ğŸŒ¦ï¸ Pipeline Weather

Este projeto Ã© uma pipeline de dados climÃ¡ticos que coleta, processa e armazena informaÃ§Ãµes meteorolÃ³gicas para cidades especÃ­ficas, utilizando Apache Airflow e PostgreSQL.

O objetivo Ã© centralizar dados de clima em um banco de dados relacional, permitindo anÃ¡lises e visualizaÃ§Ãµes futuras.

ğŸ§© Tecnologias utilizadas

Python 3.x â€“ Linguagem principal do projeto

Apache Airflow 3.x â€“ OrquestraÃ§Ã£o da pipeline

PostgreSQL 16 â€“ Banco de dados para armazenamento dos dados

Docker & Docker Compose â€“ ContainerizaÃ§Ã£o da aplicaÃ§Ã£o

Redis â€“ Broker para CeleryExecutor (Airflow)

âš™ï¸ Estrutura do projeto
pipeline_weather/
â”‚
â”œâ”€â”€ dags/                  # DAGs do Airflow
â”‚   â””â”€â”€ weather_dag.py
â”œâ”€â”€ src/                   # Scripts de extraÃ§Ã£o, transformaÃ§Ã£o e carregamento
â”‚   â”œâ”€â”€ extract_data.py
â”‚   â”œâ”€â”€ transform_data.py
â”‚   â””â”€â”€ load_data.py
â”œâ”€â”€ logs/                  # Logs do Airflow
â”œâ”€â”€ plugins/               # Plugins do Airflow (opcional)
â”œâ”€â”€ docker-compose.yaml    # ConfiguraÃ§Ã£o dos containers
â””â”€â”€ README.md              # DocumentaÃ§Ã£o do projeto
ğŸš€ Como rodar o projeto
1. Clonar o repositÃ³rio
git clone https://github.com/GabrielP1nheiro/pipeline_weather.git
cd pipeline_weather
2. Subir containers com Docker Compose
docker compose up -d

Isso vai subir:

Airflow (Scheduler, Webserver, Worker, DAG Processor, Triggerer)

PostgreSQL

Redis

3. Acessar o Airflow

Abra o navegador e acesse:

http://localhost:8080

UsuÃ¡rio e senha padrÃ£o:

Username: airflow
Password: airflow
4. Acessar o PostgreSQL

VocÃª pode acessar o banco de dados weather_data via psql ou pgAdmin:

# Exemplo usando psql dentro do container Airflow Worker
docker exec -it pipeline_weather-airflow-worker-1 bash
psql -h host.docker.internal -p 5432 -U airflow -d weather_data

Use o mesmo host e porta no pgAdmin para se conectar:
Host: localhost | Porta: 5432 | UsuÃ¡rio: airflow | Senha: airflow

ğŸ“¦ Como funciona a pipeline

ExtraÃ§Ã£o â€“ O script extract_data.py coleta dados climÃ¡ticos de APIs externas.

TransformaÃ§Ã£o / NormalizaÃ§Ã£o â€“ Ajusta dados, formata timestamps e converte tipos.

Carregamento â€“ Salva os dados processados no banco weather_data (tabela sp_weather).

As tarefas estÃ£o orquestradas no DAG weather_dag.py.

ğŸ“ ObservaÃ§Ãµes

Certifique-se de que o .gitignore esteja configurado para nÃ£o subir logs ou credenciais.

A pipeline estÃ¡ configurada para rodar localmente via Docker; ajustes sÃ£o necessÃ¡rios para produÃ§Ã£o.

AlteraÃ§Ãµes nas credenciais do PostgreSQL devem ser refletidas no docker-compose.yaml.
